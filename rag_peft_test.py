import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_models import ChatOllama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from huggingface_hub import login

# PDF íŒŒì¼ ë¡œë“œ ë° ë¶„í• 
loader = PyPDFLoader("data\ì»´í“¨í„°ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼.pdf")
pages = loader.load_and_split()

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(pages)

# HuggingFace Embeddings ì„¤ì •
token_key = "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"
login(token=token_key)
model_name = "jhgan/ko-sbert-nli"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
docsearch = Chroma.from_documents(texts, hf)
retriever = docsearch.as_retriever(search_type="mmr", search_kwargs={'k': 3, 'fetch_k': 10})

# ChatPromptTemplate ì„¤ì •
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# Streamlit ì„¤ì •
st.set_page_config(page_title="í”¼ë“œë°± ë°ëª¨", page_icon="ğŸš€")
st.title("ğŸš€ (peft+rag)customized modelğŸš€")

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜
def print_message():
    if 'messages' in st.session_state and len(st.session_state['messages']) > 0:
        for chatmessage in st.session_state['messages']:
            st.chat_message(chatmessage.role).write(chatmessage.content)

# ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]

print_message()

# ì‚¬ìš©ìì˜ ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš© ì¶œë ¥
    st.chat_message("user").write(f"{user_input}")
    st.session_state['messages'].append(ChatMessage(role='user', content=user_input))

    # LLM ì„¤ì •
    llm = ChatOllama(model="Llama3_ko_8b_q5:latest")

    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ contextë¡œ ì„¤ì •
    context_documents = retriever.get_relevant_documents(user_input)
    context = "\n".join([doc.page_content for doc in context_documents])

    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì²´ì¸ ìƒì„±
    chain = prompt | llm
    chain_with_memory = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="history",
    )

    # ì²´ì¸ ì‹¤í–‰
    response = chain_with_memory.invoke(
        {"context": context, "question": user_input},
        config={"configurable": {"session_id": "abc123"}}
    )
    msg = response.content

    # AI ë‹µë³€ ì¶œë ¥
    with st.chat_message("assistant"):
        st.write(msg)
        st.session_state['messages'].append(ChatMessage(role='assistant', content=msg))
